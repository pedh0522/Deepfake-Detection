{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftqRPKr8S2F5"
      },
      "source": [
        "\n",
        "Mounting google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N69lzC5UDHI",
        "outputId": "957795f1-3ca9-45ac-d747-64dd9862f39f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyBsmiXNd6U3"
      },
      "source": [
        "'/content/drive/MyDrive/DeepFake dta/diffusion_datasets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8X-CdgzbJ0M",
        "outputId": "d6ff60c4-101d-48f6-dbb7-584a1743cb2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-qhi__3e_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-qhi__3e_\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=74498233b8fc0e10e6bef68bfdf3c37263dc66e65242cac37d911c540b613fd2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mr4683vf/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJM6nJP3S77O"
      },
      "source": [
        "Load CLIP-ViT-L/14 model to encode images, preprocessing images with gaussisan blur and JPEG compression and encode them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLOIMAqGRlUy",
        "outputId": "04d163d6-6d4a-46d8-8ca4-607f0355313b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 890M/890M [00:10<00:00, 92.8MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter\n",
        "import io\n",
        "import clip\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, transform = clip.load(\"ViT-L/14\", device=device)\n",
        "\n",
        "# Function for Gaussian blur and JPEG compression\n",
        "def preprocess(image, blur_radius=5, jpeg_quality=85):\n",
        "    blurred_image = cv2.GaussianBlur(image, (blur_radius, blur_radius), 0)\n",
        "    pil_image = Image.fromarray(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n",
        "    image_bytes = io.BytesIO()\n",
        "    pil_image.save(image_bytes, 'JPEG', quality=jpeg_quality)\n",
        "    return image_bytes.getvalue()\n",
        "\n",
        "# Function to encode images one by one\n",
        "def load_encode_recursive(folder_path, model, transform):\n",
        "    features_list, labels_list, fake_list = [], [], []\n",
        "\n",
        "    for class_label, class_name in enumerate(['real', 'fake']):\n",
        "        class_folder = os.path.join(folder_path, class_name)\n",
        "\n",
        "        for root, dirs, files in os.walk(class_folder):\n",
        "            for filename in files:\n",
        "                image_path = os.path.join(root, filename)\n",
        "\n",
        "                image = cv2.imread(image_path)\n",
        "                preprocessed_image = preprocess(image)\n",
        "\n",
        "                preprocessed_pil_image = Image.open(io.BytesIO(preprocessed_image))\n",
        "                transformed_image = transform(preprocessed_pil_image).unsqueeze(0).to(device)\n",
        "\n",
        "                features = model.encode_image(transformed_image)\n",
        "                features_np = features.detach().cpu().numpy()\n",
        "\n",
        "                features_list.append(features_np)\n",
        "                labels_list.append([class_label, root])\n",
        "\n",
        "    return np.concatenate(features_list), np.array(labels_list), fake_list\n",
        "\n",
        "\n",
        "# Load, preprocess, and encode images\n",
        "all_features, all_labels, fake_features = load_encode_recursive('/content/drive/MyDrive/Deepfake', model, transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmvEIepd8LFn",
        "outputId": "9ad5e1d8-9060-4480-946c-984a1aecd770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "labels = [int(all_labels[i][0]) for i in range(len(all_labels))]\n",
        "classes = [all_labels[i][1] for i in range(len(all_labels))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwpjHxpLTXR9"
      },
      "source": [
        "Adding encodings of deepfake images of different generative models for training SVMs, Random Forest and Neural Network for chain classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2efuB1M7-sGB"
      },
      "outputs": [],
      "source": [
        "uniq_class = set(classes)\n",
        "uniq_class1 = []\n",
        "ll = ['/content/drive/MyDrive/Deepfake/fake/fake1', '/content/drive/MyDrive/Deepfake/fake/fake6', '/content/drive/MyDrive/Deepfake/fake/fake2/1_fake', '/content/drive/MyDrive/Deepfake/fake/fake7', '/content/drive/MyDrive/Deepfake/fake/fake3', '/content/drive/MyDrive/Deepfake/fake/fake2', '/content/drive/MyDrive/Deepfake/fake/fake4', '/content/drive/MyDrive/Deepfake/fake/fake5']\n",
        "for clss in uniq_class:\n",
        "  if clss in ll:\n",
        "    uniq_class1.append(clss)\n",
        "feat_dict = {}\n",
        "for clss in uniq_class1:\n",
        "  feat_dict[clss] = []\n",
        "for ind, feature in enumerate(all_features):\n",
        "  if all_labels[ind][1] in uniq_class1:\n",
        "    feat_dict[all_labels[ind][1]].append(feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8a_zr5TJUm"
      },
      "source": [
        "Adding a layer of Neural Network to classify image as real or fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaMcUgW3frt4",
        "outputId": "49a5d90c-3e4a-4c90-f776-df1873625a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Loss: 0.8863220810890198\n",
            "Epoch [2/1000], Loss: 0.8139565587043762\n",
            "Epoch [3/1000], Loss: 0.7476633787155151\n",
            "Epoch [4/1000], Loss: 0.6876567602157593\n",
            "Epoch [5/1000], Loss: 0.6340117454528809\n",
            "Epoch [6/1000], Loss: 0.5866555571556091\n",
            "Epoch [7/1000], Loss: 0.5453728437423706\n",
            "Epoch [8/1000], Loss: 0.5098185539245605\n",
            "Epoch [9/1000], Loss: 0.47954341769218445\n",
            "Epoch [10/1000], Loss: 0.45402559638023376\n",
            "Epoch [11/1000], Loss: 0.4327049255371094\n",
            "Epoch [12/1000], Loss: 0.4150128960609436\n",
            "Epoch [13/1000], Loss: 0.40039774775505066\n",
            "Epoch [14/1000], Loss: 0.3883429765701294\n",
            "Epoch [15/1000], Loss: 0.37838003039360046\n",
            "Epoch [16/1000], Loss: 0.3700948655605316\n",
            "Epoch [17/1000], Loss: 0.36313027143478394\n",
            "Epoch [18/1000], Loss: 0.35718441009521484\n",
            "Epoch [19/1000], Loss: 0.35200726985931396\n",
            "Epoch [20/1000], Loss: 0.34739547967910767\n",
            "Epoch [21/1000], Loss: 0.3431870937347412\n",
            "Epoch [22/1000], Loss: 0.33925575017929077\n",
            "Epoch [23/1000], Loss: 0.33550530672073364\n",
            "Epoch [24/1000], Loss: 0.33186501264572144\n",
            "Epoch [25/1000], Loss: 0.3282849192619324\n",
            "Epoch [26/1000], Loss: 0.3247322142124176\n",
            "Epoch [27/1000], Loss: 0.32118746638298035\n",
            "Epoch [28/1000], Loss: 0.31764209270477295\n",
            "Epoch [29/1000], Loss: 0.3140956163406372\n",
            "Epoch [30/1000], Loss: 0.310553640127182\n",
            "Epoch [31/1000], Loss: 0.3070259690284729\n",
            "Epoch [32/1000], Loss: 0.3035252094268799\n",
            "Epoch [33/1000], Loss: 0.3000655770301819\n",
            "Epoch [34/1000], Loss: 0.29666152596473694\n",
            "Epoch [35/1000], Loss: 0.2933272123336792\n",
            "Epoch [36/1000], Loss: 0.29007580876350403\n",
            "Epoch [37/1000], Loss: 0.2869187891483307\n",
            "Epoch [38/1000], Loss: 0.28386566042900085\n",
            "Epoch [39/1000], Loss: 0.28092387318611145\n",
            "Epoch [40/1000], Loss: 0.2780984044075012\n",
            "Epoch [41/1000], Loss: 0.2753918468952179\n",
            "Epoch [42/1000], Loss: 0.27280473709106445\n",
            "Epoch [43/1000], Loss: 0.2703353464603424\n",
            "Epoch [44/1000], Loss: 0.26798006892204285\n",
            "Epoch [45/1000], Loss: 0.26573365926742554\n",
            "Epoch [46/1000], Loss: 0.26358968019485474\n",
            "Epoch [47/1000], Loss: 0.2615405321121216\n",
            "Epoch [48/1000], Loss: 0.25957804918289185\n",
            "Epoch [49/1000], Loss: 0.2576937675476074\n",
            "Epoch [50/1000], Loss: 0.25587913393974304\n",
            "Epoch [51/1000], Loss: 0.25412580370903015\n",
            "Epoch [52/1000], Loss: 0.252425879240036\n",
            "Epoch [53/1000], Loss: 0.2507721185684204\n",
            "Epoch [54/1000], Loss: 0.2491580843925476\n",
            "Epoch [55/1000], Loss: 0.24757805466651917\n",
            "Epoch [56/1000], Loss: 0.24602735042572021\n",
            "Epoch [57/1000], Loss: 0.24450208246707916\n",
            "Epoch [58/1000], Loss: 0.24299919605255127\n",
            "Epoch [59/1000], Loss: 0.24151648581027985\n",
            "Epoch [60/1000], Loss: 0.2400524616241455\n",
            "Epoch [61/1000], Loss: 0.23860612511634827\n",
            "Epoch [62/1000], Loss: 0.23717710375785828\n",
            "Epoch [63/1000], Loss: 0.23576533794403076\n",
            "Epoch [64/1000], Loss: 0.2343711107969284\n",
            "Epoch [65/1000], Loss: 0.23299483954906464\n",
            "Epoch [66/1000], Loss: 0.23163709044456482\n",
            "Epoch [67/1000], Loss: 0.23029841482639313\n",
            "Epoch [68/1000], Loss: 0.22897937893867493\n",
            "Epoch [69/1000], Loss: 0.22768044471740723\n",
            "Epoch [70/1000], Loss: 0.22640196979045868\n",
            "Epoch [71/1000], Loss: 0.2251441776752472\n",
            "Epoch [72/1000], Loss: 0.22390714287757874\n",
            "Epoch [73/1000], Loss: 0.2226908653974533\n",
            "Epoch [74/1000], Loss: 0.221495121717453\n",
            "Epoch [75/1000], Loss: 0.22031961381435394\n",
            "Epoch [76/1000], Loss: 0.21916398406028748\n",
            "Epoch [77/1000], Loss: 0.21802766621112823\n",
            "Epoch [78/1000], Loss: 0.21691015362739563\n",
            "Epoch [79/1000], Loss: 0.2158108651638031\n",
            "Epoch [80/1000], Loss: 0.2147291749715805\n",
            "Epoch [81/1000], Loss: 0.2136644870042801\n",
            "Epoch [82/1000], Loss: 0.21261616051197052\n",
            "Epoch [83/1000], Loss: 0.21158362925052643\n",
            "Epoch [84/1000], Loss: 0.21056634187698364\n",
            "Epoch [85/1000], Loss: 0.20956376194953918\n",
            "Epoch [86/1000], Loss: 0.20857541263103485\n",
            "Epoch [87/1000], Loss: 0.20760086178779602\n",
            "Epoch [88/1000], Loss: 0.20663969218730927\n",
            "Epoch [89/1000], Loss: 0.20569159090518951\n",
            "Epoch [90/1000], Loss: 0.20475620031356812\n",
            "Epoch [91/1000], Loss: 0.20383326709270477\n",
            "Epoch [92/1000], Loss: 0.2029224932193756\n",
            "Epoch [93/1000], Loss: 0.2020236998796463\n",
            "Epoch [94/1000], Loss: 0.20113663375377655\n",
            "Epoch [95/1000], Loss: 0.20026111602783203\n",
            "Epoch [96/1000], Loss: 0.19939696788787842\n",
            "Epoch [97/1000], Loss: 0.198543980717659\n",
            "Epoch [98/1000], Loss: 0.19770199060440063\n",
            "Epoch [99/1000], Loss: 0.1968708336353302\n",
            "Epoch [100/1000], Loss: 0.19605031609535217\n",
            "Epoch [101/1000], Loss: 0.19524027407169342\n",
            "Epoch [102/1000], Loss: 0.19444052875041962\n",
            "Epoch [103/1000], Loss: 0.19365090131759644\n",
            "Epoch [104/1000], Loss: 0.19287119805812836\n",
            "Epoch [105/1000], Loss: 0.19210126996040344\n",
            "Epoch [106/1000], Loss: 0.19134089350700378\n",
            "Epoch [107/1000], Loss: 0.19058988988399506\n",
            "Epoch [108/1000], Loss: 0.18984808027744293\n",
            "Epoch [109/1000], Loss: 0.18911530077457428\n",
            "Epoch [110/1000], Loss: 0.18839137256145477\n",
            "Epoch [111/1000], Loss: 0.1876761019229889\n",
            "Epoch [112/1000], Loss: 0.1869693100452423\n",
            "Epoch [113/1000], Loss: 0.1862708479166031\n",
            "Epoch [114/1000], Loss: 0.1855805516242981\n",
            "Epoch [115/1000], Loss: 0.1848982572555542\n",
            "Epoch [116/1000], Loss: 0.18422383069992065\n",
            "Epoch [117/1000], Loss: 0.18355710804462433\n",
            "Epoch [118/1000], Loss: 0.18289795517921448\n",
            "Epoch [119/1000], Loss: 0.18224622309207916\n",
            "Epoch [120/1000], Loss: 0.18160177767276764\n",
            "Epoch [121/1000], Loss: 0.18096454441547394\n",
            "Epoch [122/1000], Loss: 0.18033431470394135\n",
            "Epoch [123/1000], Loss: 0.17971104383468628\n",
            "Epoch [124/1000], Loss: 0.17909455299377441\n",
            "Epoch [125/1000], Loss: 0.17848478257656097\n",
            "Epoch [126/1000], Loss: 0.17788156867027283\n",
            "Epoch [127/1000], Loss: 0.1772848665714264\n",
            "Epoch [128/1000], Loss: 0.17669451236724854\n",
            "Epoch [129/1000], Loss: 0.1761104315519333\n",
            "Epoch [130/1000], Loss: 0.1755325049161911\n",
            "Epoch [131/1000], Loss: 0.1749606728553772\n",
            "Epoch [132/1000], Loss: 0.17439478635787964\n",
            "Epoch [133/1000], Loss: 0.17383478581905365\n",
            "Epoch [134/1000], Loss: 0.17328055202960968\n",
            "Epoch [135/1000], Loss: 0.17273204028606415\n",
            "Epoch [136/1000], Loss: 0.17218910157680511\n",
            "Epoch [137/1000], Loss: 0.17165164649486542\n",
            "Epoch [138/1000], Loss: 0.17111963033676147\n",
            "Epoch [139/1000], Loss: 0.17059296369552612\n",
            "Epoch [140/1000], Loss: 0.170071542263031\n",
            "Epoch [141/1000], Loss: 0.16955527663230896\n",
            "Epoch [142/1000], Loss: 0.1690441071987152\n",
            "Epoch [143/1000], Loss: 0.1685379445552826\n",
            "Epoch [144/1000], Loss: 0.16803668439388275\n",
            "Epoch [145/1000], Loss: 0.16754032671451569\n",
            "Epoch [146/1000], Loss: 0.16704870760440826\n",
            "Epoch [147/1000], Loss: 0.16656184196472168\n",
            "Epoch [148/1000], Loss: 0.166079580783844\n",
            "Epoch [149/1000], Loss: 0.165601909160614\n",
            "Epoch [150/1000], Loss: 0.16512873768806458\n",
            "Epoch [151/1000], Loss: 0.16465997695922852\n",
            "Epoch [152/1000], Loss: 0.16419561207294464\n",
            "Epoch [153/1000], Loss: 0.1637355238199234\n",
            "Epoch [154/1000], Loss: 0.1632796972990036\n",
            "Epoch [155/1000], Loss: 0.16282805800437927\n",
            "Epoch [156/1000], Loss: 0.16238053143024445\n",
            "Epoch [157/1000], Loss: 0.16193704307079315\n",
            "Epoch [158/1000], Loss: 0.1614975929260254\n",
            "Epoch [159/1000], Loss: 0.1610620766878128\n",
            "Epoch [160/1000], Loss: 0.16063043475151062\n",
            "Epoch [161/1000], Loss: 0.16020263731479645\n",
            "Epoch [162/1000], Loss: 0.1597786396741867\n",
            "Epoch [163/1000], Loss: 0.15935833752155304\n",
            "Epoch [164/1000], Loss: 0.15894174575805664\n",
            "Epoch [165/1000], Loss: 0.15852876007556915\n",
            "Epoch [166/1000], Loss: 0.158119335770607\n",
            "Epoch [167/1000], Loss: 0.15771345794200897\n",
            "Epoch [168/1000], Loss: 0.15731103718280792\n",
            "Epoch [169/1000], Loss: 0.15691207349300385\n",
            "Epoch [170/1000], Loss: 0.15651647746562958\n",
            "Epoch [171/1000], Loss: 0.15612423419952393\n",
            "Epoch [172/1000], Loss: 0.15573526918888092\n",
            "Epoch [173/1000], Loss: 0.15534953773021698\n",
            "Epoch [174/1000], Loss: 0.1549670398235321\n",
            "Epoch [175/1000], Loss: 0.15458770096302032\n",
            "Epoch [176/1000], Loss: 0.15421146154403687\n",
            "Epoch [177/1000], Loss: 0.15383830666542053\n",
            "Epoch [178/1000], Loss: 0.15346820652484894\n",
            "Epoch [179/1000], Loss: 0.1531011164188385\n",
            "Epoch [180/1000], Loss: 0.15273697674274445\n",
            "Epoch [181/1000], Loss: 0.1523757427930832\n",
            "Epoch [182/1000], Loss: 0.15201741456985474\n",
            "Epoch [183/1000], Loss: 0.1516619175672531\n",
            "Epoch [184/1000], Loss: 0.15130923688411713\n",
            "Epoch [185/1000], Loss: 0.15095935761928558\n",
            "Epoch [186/1000], Loss: 0.15061219036579132\n",
            "Epoch [187/1000], Loss: 0.15026773512363434\n",
            "Epoch [188/1000], Loss: 0.14992597699165344\n",
            "Epoch [189/1000], Loss: 0.14958682656288147\n",
            "Epoch [190/1000], Loss: 0.14925029873847961\n",
            "Epoch [191/1000], Loss: 0.1489163488149643\n",
            "Epoch [192/1000], Loss: 0.14858491718769073\n",
            "Epoch [193/1000], Loss: 0.14825603365898132\n",
            "Epoch [194/1000], Loss: 0.1479296088218689\n",
            "Epoch [195/1000], Loss: 0.14760564267635345\n",
            "Epoch [196/1000], Loss: 0.14728409051895142\n",
            "Epoch [197/1000], Loss: 0.1469649374485016\n",
            "Epoch [198/1000], Loss: 0.14664813876152039\n",
            "Epoch [199/1000], Loss: 0.14633367955684662\n",
            "Epoch [200/1000], Loss: 0.1460215300321579\n",
            "Epoch [201/1000], Loss: 0.14571167528629303\n",
            "Epoch [202/1000], Loss: 0.14540404081344604\n",
            "Epoch [203/1000], Loss: 0.14509867131710052\n",
            "Epoch [204/1000], Loss: 0.1447954773902893\n",
            "Epoch [205/1000], Loss: 0.1444944590330124\n",
            "Epoch [206/1000], Loss: 0.14419560134410858\n",
            "Epoch [207/1000], Loss: 0.1438988447189331\n",
            "Epoch [208/1000], Loss: 0.14360418915748596\n",
            "Epoch [209/1000], Loss: 0.14331161975860596\n",
            "Epoch [210/1000], Loss: 0.1430211216211319\n",
            "Epoch [211/1000], Loss: 0.1427326202392578\n",
            "Epoch [212/1000], Loss: 0.1424461454153061\n",
            "Epoch [213/1000], Loss: 0.14216162264347076\n",
            "Epoch [214/1000], Loss: 0.1418790966272354\n",
            "Epoch [215/1000], Loss: 0.14159849286079407\n",
            "Epoch [216/1000], Loss: 0.14131979644298553\n",
            "Epoch [217/1000], Loss: 0.14104300737380981\n",
            "Epoch [218/1000], Loss: 0.1407681107521057\n",
            "Epoch [219/1000], Loss: 0.14049503207206726\n",
            "Epoch [220/1000], Loss: 0.14022380113601685\n",
            "Epoch [221/1000], Loss: 0.13995438814163208\n",
            "Epoch [222/1000], Loss: 0.13968677818775177\n",
            "Epoch [223/1000], Loss: 0.13942091166973114\n",
            "Epoch [224/1000], Loss: 0.13915680348873138\n",
            "Epoch [225/1000], Loss: 0.1388944536447525\n",
            "Epoch [226/1000], Loss: 0.13863380253314972\n",
            "Epoch [227/1000], Loss: 0.13837485015392303\n",
            "Epoch [228/1000], Loss: 0.13811759650707245\n",
            "Epoch [229/1000], Loss: 0.13786198198795319\n",
            "Epoch [230/1000], Loss: 0.13760800659656525\n",
            "Epoch [231/1000], Loss: 0.13735568523406982\n",
            "Epoch [232/1000], Loss: 0.13710497319698334\n",
            "Epoch [233/1000], Loss: 0.1368558257818222\n",
            "Epoch [234/1000], Loss: 0.1366082727909088\n",
            "Epoch [235/1000], Loss: 0.13636228442192078\n",
            "Epoch [236/1000], Loss: 0.1361178308725357\n",
            "Epoch [237/1000], Loss: 0.1358748972415924\n",
            "Epoch [238/1000], Loss: 0.13563348352909088\n",
            "Epoch [239/1000], Loss: 0.13539357483386993\n",
            "Epoch [240/1000], Loss: 0.13515512645244598\n",
            "Epoch [241/1000], Loss: 0.13491816818714142\n",
            "Epoch [242/1000], Loss: 0.13468265533447266\n",
            "Epoch [243/1000], Loss: 0.1344485878944397\n",
            "Epoch [244/1000], Loss: 0.13421592116355896\n",
            "Epoch [245/1000], Loss: 0.13398467004299164\n",
            "Epoch [246/1000], Loss: 0.13375480473041534\n",
            "Epoch [247/1000], Loss: 0.13352634012699127\n",
            "Epoch [248/1000], Loss: 0.13329923152923584\n",
            "Epoch [249/1000], Loss: 0.13307346403598785\n",
            "Epoch [250/1000], Loss: 0.13284903764724731\n",
            "Epoch [251/1000], Loss: 0.13262595236301422\n",
            "Epoch [252/1000], Loss: 0.1324041485786438\n",
            "Epoch [253/1000], Loss: 0.13218368589878082\n",
            "Epoch [254/1000], Loss: 0.13196448981761932\n",
            "Epoch [255/1000], Loss: 0.1317465752363205\n",
            "Epoch [256/1000], Loss: 0.13152991235256195\n",
            "Epoch [257/1000], Loss: 0.1313144862651825\n",
            "Epoch [258/1000], Loss: 0.13110032677650452\n",
            "Epoch [259/1000], Loss: 0.13088738918304443\n",
            "Epoch [260/1000], Loss: 0.13067565858364105\n",
            "Epoch [261/1000], Loss: 0.13046513497829437\n",
            "Epoch [262/1000], Loss: 0.1302558034658432\n",
            "Epoch [263/1000], Loss: 0.13004764914512634\n",
            "Epoch [264/1000], Loss: 0.1298406720161438\n",
            "Epoch [265/1000], Loss: 0.12963484227657318\n",
            "Epoch [266/1000], Loss: 0.1294301599264145\n",
            "Epoch [267/1000], Loss: 0.12922662496566772\n",
            "Epoch [268/1000], Loss: 0.1290242075920105\n",
            "Epoch [269/1000], Loss: 0.1288229078054428\n",
            "Epoch [270/1000], Loss: 0.12862272560596466\n",
            "Epoch [271/1000], Loss: 0.12842361629009247\n",
            "Epoch [272/1000], Loss: 0.12822562456130981\n",
            "Epoch [273/1000], Loss: 0.12802867591381073\n",
            "Epoch [274/1000], Loss: 0.1278328001499176\n",
            "Epoch [275/1000], Loss: 0.12763799726963043\n",
            "Epoch [276/1000], Loss: 0.12744422256946564\n",
            "Epoch [277/1000], Loss: 0.1272515058517456\n",
            "Epoch [278/1000], Loss: 0.12705978751182556\n",
            "Epoch [279/1000], Loss: 0.1268690973520279\n",
            "Epoch [280/1000], Loss: 0.1266794353723526\n",
            "Epoch [281/1000], Loss: 0.1264907568693161\n",
            "Epoch [282/1000], Loss: 0.1263030618429184\n",
            "Epoch [283/1000], Loss: 0.12611635029315948\n",
            "Epoch [284/1000], Loss: 0.12593063712120056\n",
            "Epoch [285/1000], Loss: 0.12574587762355804\n",
            "Epoch [286/1000], Loss: 0.12556205689907074\n",
            "Epoch [287/1000], Loss: 0.12537920475006104\n",
            "Epoch [288/1000], Loss: 0.12519729137420654\n",
            "Epoch [289/1000], Loss: 0.12501631677150726\n",
            "Epoch [290/1000], Loss: 0.12483625113964081\n",
            "Epoch [291/1000], Loss: 0.12465709447860718\n",
            "Epoch [292/1000], Loss: 0.12447885423898697\n",
            "Epoch [293/1000], Loss: 0.12430153042078018\n",
            "Epoch [294/1000], Loss: 0.12412508577108383\n",
            "Epoch [295/1000], Loss: 0.12394952774047852\n",
            "Epoch [296/1000], Loss: 0.12377484142780304\n",
            "Epoch [297/1000], Loss: 0.1236010193824768\n",
            "Epoch [298/1000], Loss: 0.12342807650566101\n",
            "Epoch [299/1000], Loss: 0.12325599044561386\n",
            "Epoch [300/1000], Loss: 0.12308473885059357\n",
            "Epoch [301/1000], Loss: 0.12291432917118073\n",
            "Epoch [302/1000], Loss: 0.12274475395679474\n",
            "Epoch [303/1000], Loss: 0.1225760206580162\n",
            "Epoch [304/1000], Loss: 0.12240808457136154\n",
            "Epoch [305/1000], Loss: 0.12224098294973373\n",
            "Epoch [306/1000], Loss: 0.1220746859908104\n",
            "Epoch [307/1000], Loss: 0.12190917134284973\n",
            "Epoch [308/1000], Loss: 0.12174446135759354\n",
            "Epoch [309/1000], Loss: 0.12158054113388062\n",
            "Epoch [310/1000], Loss: 0.12141740322113037\n",
            "Epoch [311/1000], Loss: 0.12125503271818161\n",
            "Epoch [312/1000], Loss: 0.12109343707561493\n",
            "Epoch [313/1000], Loss: 0.12093259394168854\n",
            "Epoch [314/1000], Loss: 0.12077251076698303\n",
            "Epoch [315/1000], Loss: 0.12061318010091782\n",
            "Epoch [316/1000], Loss: 0.1204545795917511\n",
            "Epoch [317/1000], Loss: 0.12029673159122467\n",
            "Epoch [318/1000], Loss: 0.12013960629701614\n",
            "Epoch [319/1000], Loss: 0.11998321115970612\n",
            "Epoch [320/1000], Loss: 0.11982753872871399\n",
            "Epoch [321/1000], Loss: 0.11967258155345917\n",
            "Epoch [322/1000], Loss: 0.11951832473278046\n",
            "Epoch [323/1000], Loss: 0.11936478316783905\n",
            "Epoch [324/1000], Loss: 0.11921192705631256\n",
            "Epoch [325/1000], Loss: 0.11905977129936218\n",
            "Epoch [326/1000], Loss: 0.11890830099582672\n",
            "Epoch [327/1000], Loss: 0.11875751614570618\n",
            "Epoch [328/1000], Loss: 0.11860739439725876\n",
            "Epoch [329/1000], Loss: 0.11845795065164566\n",
            "Epoch [330/1000], Loss: 0.11830917745828629\n",
            "Epoch [331/1000], Loss: 0.11816105991601944\n",
            "Epoch [332/1000], Loss: 0.11801359802484512\n",
            "Epoch [333/1000], Loss: 0.11786678433418274\n",
            "Epoch [334/1000], Loss: 0.11772062629461288\n",
            "Epoch [335/1000], Loss: 0.11757509410381317\n",
            "Epoch [336/1000], Loss: 0.11743021011352539\n",
            "Epoch [337/1000], Loss: 0.11728595197200775\n",
            "Epoch [338/1000], Loss: 0.11714232712984085\n",
            "Epoch [339/1000], Loss: 0.1169993132352829\n",
            "Epoch [340/1000], Loss: 0.11685692518949509\n",
            "Epoch [341/1000], Loss: 0.11671514064073563\n",
            "Epoch [342/1000], Loss: 0.11657397449016571\n",
            "Epoch [343/1000], Loss: 0.11643340438604355\n",
            "Epoch [344/1000], Loss: 0.11629343032836914\n",
            "Epoch [345/1000], Loss: 0.11615405976772308\n",
            "Epoch [346/1000], Loss: 0.11601527035236359\n",
            "Epoch [347/1000], Loss: 0.11587706953287125\n",
            "Epoch [348/1000], Loss: 0.11573945730924606\n",
            "Epoch [349/1000], Loss: 0.11560240387916565\n",
            "Epoch [350/1000], Loss: 0.11546593904495239\n",
            "Epoch [351/1000], Loss: 0.1153300479054451\n",
            "Epoch [352/1000], Loss: 0.11519470810890198\n",
            "Epoch [353/1000], Loss: 0.11505993455648422\n",
            "Epoch [354/1000], Loss: 0.11492571234703064\n",
            "Epoch [355/1000], Loss: 0.11479204148054123\n",
            "Epoch [356/1000], Loss: 0.11465892940759659\n",
            "Epoch [357/1000], Loss: 0.11452635377645493\n",
            "Epoch [358/1000], Loss: 0.11439432948827744\n",
            "Epoch [359/1000], Loss: 0.11426281929016113\n",
            "Epoch [360/1000], Loss: 0.1141318753361702\n",
            "Epoch [361/1000], Loss: 0.11400143802165985\n",
            "Epoch [362/1000], Loss: 0.11387152969837189\n",
            "Epoch [363/1000], Loss: 0.11374214291572571\n",
            "Epoch [364/1000], Loss: 0.11361327022314072\n",
            "Epoch [365/1000], Loss: 0.11348491907119751\n",
            "Epoch [366/1000], Loss: 0.1133570745587349\n",
            "Epoch [367/1000], Loss: 0.11322974413633347\n",
            "Epoch [368/1000], Loss: 0.11310291290283203\n",
            "Epoch [369/1000], Loss: 0.11297658085823059\n",
            "Epoch [370/1000], Loss: 0.11285074055194855\n",
            "Epoch [371/1000], Loss: 0.1127254068851471\n",
            "Epoch [372/1000], Loss: 0.11260055750608444\n",
            "Epoch [373/1000], Loss: 0.11247618496417999\n",
            "Epoch [374/1000], Loss: 0.11235231161117554\n",
            "Epoch [375/1000], Loss: 0.11222890019416809\n",
            "Epoch [376/1000], Loss: 0.11210598051548004\n",
            "Epoch [377/1000], Loss: 0.111983522772789\n",
            "Epoch [378/1000], Loss: 0.11186154931783676\n",
            "Epoch [379/1000], Loss: 0.11174002289772034\n",
            "Epoch [380/1000], Loss: 0.1116189956665039\n",
            "Epoch [381/1000], Loss: 0.1114983931183815\n",
            "Epoch [382/1000], Loss: 0.1113782599568367\n",
            "Epoch [383/1000], Loss: 0.11125858128070831\n",
            "Epoch [384/1000], Loss: 0.11113936454057693\n",
            "Epoch [385/1000], Loss: 0.11102057993412018\n",
            "Epoch [386/1000], Loss: 0.11090224236249924\n",
            "Epoch [387/1000], Loss: 0.11078435182571411\n",
            "Epoch [388/1000], Loss: 0.1106669083237648\n",
            "Epoch [389/1000], Loss: 0.11054989695549011\n",
            "Epoch [390/1000], Loss: 0.11043331027030945\n",
            "Epoch [391/1000], Loss: 0.110317163169384\n",
            "Epoch [392/1000], Loss: 0.11020144820213318\n",
            "Epoch [393/1000], Loss: 0.11008615791797638\n",
            "Epoch [394/1000], Loss: 0.10997127741575241\n",
            "Epoch [395/1000], Loss: 0.10985682159662247\n",
            "Epoch [396/1000], Loss: 0.10974279791116714\n",
            "Epoch [397/1000], Loss: 0.10962917655706406\n",
            "Epoch [398/1000], Loss: 0.1095159649848938\n",
            "Epoch [399/1000], Loss: 0.10940317064523697\n",
            "Epoch [400/1000], Loss: 0.10929077863693237\n",
            "Epoch [401/1000], Loss: 0.10917877405881882\n",
            "Epoch [402/1000], Loss: 0.10906719416379929\n",
            "Epoch [403/1000], Loss: 0.10895600914955139\n",
            "Epoch [404/1000], Loss: 0.10884521901607513\n",
            "Epoch [405/1000], Loss: 0.10873481631278992\n",
            "Epoch [406/1000], Loss: 0.10862481594085693\n",
            "Epoch [407/1000], Loss: 0.1085151880979538\n",
            "Epoch [408/1000], Loss: 0.1084059625864029\n",
            "Epoch [409/1000], Loss: 0.10829710215330124\n",
            "Epoch [410/1000], Loss: 0.10818864405155182\n",
            "Epoch [411/1000], Loss: 0.10808055847883224\n",
            "Epoch [412/1000], Loss: 0.10797283798456192\n",
            "Epoch [413/1000], Loss: 0.10786549746990204\n",
            "Epoch [414/1000], Loss: 0.1077585369348526\n",
            "Epoch [415/1000], Loss: 0.10765193402767181\n",
            "Epoch [416/1000], Loss: 0.10754570364952087\n",
            "Epoch [417/1000], Loss: 0.10743985325098038\n",
            "Epoch [418/1000], Loss: 0.10733434557914734\n",
            "Epoch [419/1000], Loss: 0.10722921043634415\n",
            "Epoch [420/1000], Loss: 0.10712442547082901\n",
            "Epoch [421/1000], Loss: 0.10701999068260193\n",
            "Epoch [422/1000], Loss: 0.1069159284234047\n",
            "Epoch [423/1000], Loss: 0.10681220144033432\n",
            "Epoch [424/1000], Loss: 0.1067088395357132\n",
            "Epoch [425/1000], Loss: 0.10660581290721893\n",
            "Epoch [426/1000], Loss: 0.10650314390659332\n",
            "Epoch [427/1000], Loss: 0.10640081018209457\n",
            "Epoch [428/1000], Loss: 0.10629881918430328\n",
            "Epoch [429/1000], Loss: 0.10619715601205826\n",
            "Epoch [430/1000], Loss: 0.10609584301710129\n",
            "Epoch [431/1000], Loss: 0.10599486529827118\n",
            "Epoch [432/1000], Loss: 0.10589422285556793\n",
            "Epoch [433/1000], Loss: 0.10579390823841095\n",
            "Epoch [434/1000], Loss: 0.10569392144680023\n",
            "Epoch [435/1000], Loss: 0.10559425503015518\n",
            "Epoch [436/1000], Loss: 0.10549493134021759\n",
            "Epoch [437/1000], Loss: 0.10539592057466507\n",
            "Epoch [438/1000], Loss: 0.10529723018407822\n",
            "Epoch [439/1000], Loss: 0.10519887506961823\n",
            "Epoch [440/1000], Loss: 0.10510081797838211\n",
            "Epoch [441/1000], Loss: 0.10500308871269226\n",
            "Epoch [442/1000], Loss: 0.10490565747022629\n",
            "Epoch [443/1000], Loss: 0.10480855405330658\n",
            "Epoch [444/1000], Loss: 0.10471175611019135\n",
            "Epoch [445/1000], Loss: 0.10461527109146118\n",
            "Epoch [446/1000], Loss: 0.10451909154653549\n",
            "Epoch [447/1000], Loss: 0.10442321002483368\n",
            "Epoch [448/1000], Loss: 0.10432764142751694\n",
            "Epoch [449/1000], Loss: 0.10423237830400467\n",
            "Epoch [450/1000], Loss: 0.10413739830255508\n",
            "Epoch [451/1000], Loss: 0.10404273122549057\n",
            "Epoch [452/1000], Loss: 0.10394835472106934\n",
            "Epoch [453/1000], Loss: 0.10385426878929138\n",
            "Epoch [454/1000], Loss: 0.1037604808807373\n",
            "Epoch [455/1000], Loss: 0.10366698354482651\n",
            "Epoch [456/1000], Loss: 0.10357378423213959\n",
            "Epoch [457/1000], Loss: 0.10348086059093475\n",
            "Epoch [458/1000], Loss: 0.1033882349729538\n",
            "Epoch [459/1000], Loss: 0.10329588502645493\n",
            "Epoch [460/1000], Loss: 0.10320382565259933\n",
            "Epoch [461/1000], Loss: 0.10311203449964523\n",
            "Epoch [462/1000], Loss: 0.10302054136991501\n",
            "Epoch [463/1000], Loss: 0.10292932391166687\n",
            "Epoch [464/1000], Loss: 0.10283837467432022\n",
            "Epoch [465/1000], Loss: 0.10274770110845566\n",
            "Epoch [466/1000], Loss: 0.10265731811523438\n",
            "Epoch [467/1000], Loss: 0.10256718844175339\n",
            "Epoch [468/1000], Loss: 0.10247734189033508\n",
            "Epoch [469/1000], Loss: 0.10238776355981827\n",
            "Epoch [470/1000], Loss: 0.10229843854904175\n",
            "Epoch [471/1000], Loss: 0.10220940411090851\n",
            "Epoch [472/1000], Loss: 0.10212062299251556\n",
            "Epoch [473/1000], Loss: 0.10203210264444351\n",
            "Epoch [474/1000], Loss: 0.10194384306669235\n",
            "Epoch [475/1000], Loss: 0.10185585170984268\n",
            "Epoch [476/1000], Loss: 0.1017681136727333\n",
            "Epoch [477/1000], Loss: 0.10168063640594482\n",
            "Epoch [478/1000], Loss: 0.10159341990947723\n",
            "Epoch [479/1000], Loss: 0.10150645673274994\n",
            "Epoch [480/1000], Loss: 0.10141974687576294\n",
            "Epoch [481/1000], Loss: 0.10133329033851624\n",
            "Epoch [482/1000], Loss: 0.10124708712100983\n",
            "Epoch [483/1000], Loss: 0.10116112977266312\n",
            "Epoch [484/1000], Loss: 0.10107541084289551\n",
            "Epoch [485/1000], Loss: 0.10098996013402939\n",
            "Epoch [486/1000], Loss: 0.10090475529432297\n",
            "Epoch [487/1000], Loss: 0.10081977397203445\n",
            "Epoch [488/1000], Loss: 0.10073505342006683\n",
            "Epoch [489/1000], Loss: 0.10065056383609772\n",
            "Epoch [490/1000], Loss: 0.1005663201212883\n",
            "Epoch [491/1000], Loss: 0.10048232227563858\n",
            "Epoch [492/1000], Loss: 0.10039854794740677\n",
            "Epoch [493/1000], Loss: 0.10031501948833466\n",
            "Epoch [494/1000], Loss: 0.10023172944784164\n",
            "Epoch [495/1000], Loss: 0.10014867782592773\n",
            "Epoch [496/1000], Loss: 0.10006585717201233\n",
            "Epoch [497/1000], Loss: 0.09998326003551483\n",
            "Epoch [498/1000], Loss: 0.09990090131759644\n",
            "Epoch [499/1000], Loss: 0.09981876611709595\n",
            "Epoch [500/1000], Loss: 0.09973686933517456\n",
            "Epoch [501/1000], Loss: 0.09965518862009048\n",
            "Epoch [502/1000], Loss: 0.09957375377416611\n",
            "Epoch [503/1000], Loss: 0.09949252009391785\n",
            "Epoch [504/1000], Loss: 0.09941151738166809\n",
            "Epoch [505/1000], Loss: 0.09933074563741684\n",
            "Epoch [506/1000], Loss: 0.0992501899600029\n",
            "Epoch [507/1000], Loss: 0.09916985780000687\n",
            "Epoch [508/1000], Loss: 0.09908974170684814\n",
            "Epoch [509/1000], Loss: 0.09900984913110733\n",
            "Epoch [510/1000], Loss: 0.09893016517162323\n",
            "Epoch [511/1000], Loss: 0.09885070472955704\n",
            "Epoch [512/1000], Loss: 0.09877146035432816\n",
            "Epoch [513/1000], Loss: 0.09869242459535599\n",
            "Epoch [514/1000], Loss: 0.09861359745264053\n",
            "Epoch [515/1000], Loss: 0.09853499382734299\n",
            "Epoch [516/1000], Loss: 0.09845659136772156\n",
            "Epoch [517/1000], Loss: 0.09837840497493744\n",
            "Epoch [518/1000], Loss: 0.09830043464899063\n",
            "Epoch [519/1000], Loss: 0.09822265803813934\n",
            "Epoch [520/1000], Loss: 0.09814509004354477\n",
            "Epoch [521/1000], Loss: 0.0980677381157875\n",
            "Epoch [522/1000], Loss: 0.09799057990312576\n",
            "Epoch [523/1000], Loss: 0.09791363030672073\n",
            "Epoch [524/1000], Loss: 0.09783688932657242\n",
            "Epoch [525/1000], Loss: 0.09776033461093903\n",
            "Epoch [526/1000], Loss: 0.09768399596214294\n",
            "Epoch [527/1000], Loss: 0.09760785102844238\n",
            "Epoch [528/1000], Loss: 0.09753190726041794\n",
            "Epoch [529/1000], Loss: 0.09745614975690842\n",
            "Epoch [530/1000], Loss: 0.0973806083202362\n",
            "Epoch [531/1000], Loss: 0.09730524569749832\n",
            "Epoch [532/1000], Loss: 0.09723009169101715\n",
            "Epoch [533/1000], Loss: 0.0971551239490509\n",
            "Epoch [534/1000], Loss: 0.09708034247159958\n",
            "Epoch [535/1000], Loss: 0.09700575470924377\n",
            "Epoch [536/1000], Loss: 0.09693137556314468\n",
            "Epoch [537/1000], Loss: 0.09685718268156052\n",
            "Epoch [538/1000], Loss: 0.09678316116333008\n",
            "Epoch [539/1000], Loss: 0.09670934081077576\n",
            "Epoch [540/1000], Loss: 0.09663570672273636\n",
            "Epoch [541/1000], Loss: 0.09656226634979248\n",
            "Epoch [542/1000], Loss: 0.09648900479078293\n",
            "Epoch [543/1000], Loss: 0.0964159294962883\n",
            "Epoch [544/1000], Loss: 0.096343033015728\n",
            "Epoch [545/1000], Loss: 0.09627032279968262\n",
            "Epoch [546/1000], Loss: 0.09619780629873276\n",
            "Epoch [547/1000], Loss: 0.09612545371055603\n",
            "Epoch [548/1000], Loss: 0.09605328738689423\n",
            "Epoch [549/1000], Loss: 0.09598131477832794\n",
            "Epoch [550/1000], Loss: 0.0959094986319542\n",
            "Epoch [551/1000], Loss: 0.09583787620067596\n",
            "Epoch [552/1000], Loss: 0.09576643258333206\n",
            "Epoch [553/1000], Loss: 0.09569515287876129\n",
            "Epoch [554/1000], Loss: 0.09562405943870544\n",
            "Epoch [555/1000], Loss: 0.09555313736200333\n",
            "Epoch [556/1000], Loss: 0.09548238664865494\n",
            "Epoch [557/1000], Loss: 0.09541181474924088\n",
            "Epoch [558/1000], Loss: 0.09534141421318054\n",
            "Epoch [559/1000], Loss: 0.09527118504047394\n",
            "Epoch [560/1000], Loss: 0.09520111978054047\n",
            "Epoch [561/1000], Loss: 0.09513123333454132\n",
            "Epoch [562/1000], Loss: 0.09506151080131531\n",
            "Epoch [563/1000], Loss: 0.09499195963144302\n",
            "Epoch [564/1000], Loss: 0.09492257982492447\n",
            "Epoch [565/1000], Loss: 0.09485336393117905\n",
            "Epoch [566/1000], Loss: 0.09478431940078735\n",
            "Epoch [567/1000], Loss: 0.0947154313325882\n",
            "Epoch [568/1000], Loss: 0.09464671462774277\n",
            "Epoch [569/1000], Loss: 0.09457816183567047\n",
            "Epoch [570/1000], Loss: 0.09450977295637131\n",
            "Epoch [571/1000], Loss: 0.09444154798984528\n",
            "Epoch [572/1000], Loss: 0.09437347948551178\n",
            "Epoch [573/1000], Loss: 0.09430557489395142\n",
            "Epoch [574/1000], Loss: 0.09423782676458359\n",
            "Epoch [575/1000], Loss: 0.09417024999856949\n",
            "Epoch [576/1000], Loss: 0.09410281479358673\n",
            "Epoch [577/1000], Loss: 0.0940355584025383\n",
            "Epoch [578/1000], Loss: 0.0939684510231018\n",
            "Epoch [579/1000], Loss: 0.09390149265527725\n",
            "Epoch [580/1000], Loss: 0.09383469820022583\n",
            "Epoch [581/1000], Loss: 0.09376805275678635\n",
            "Epoch [582/1000], Loss: 0.09370157122612\n",
            "Epoch [583/1000], Loss: 0.09363524615764618\n",
            "Epoch [584/1000], Loss: 0.0935690626502037\n",
            "Epoch [585/1000], Loss: 0.09350304305553436\n",
            "Epoch [586/1000], Loss: 0.09343716502189636\n",
            "Epoch [587/1000], Loss: 0.0933714509010315\n",
            "Epoch [588/1000], Loss: 0.09330588579177856\n",
            "Epoch [589/1000], Loss: 0.09324046969413757\n",
            "Epoch [590/1000], Loss: 0.09317519515752792\n",
            "Epoch [591/1000], Loss: 0.0931100845336914\n",
            "Epoch [592/1000], Loss: 0.09304510802030563\n",
            "Epoch [593/1000], Loss: 0.0929802879691124\n",
            "Epoch [594/1000], Loss: 0.0929156094789505\n",
            "Epoch [595/1000], Loss: 0.09285108000040054\n",
            "Epoch [596/1000], Loss: 0.09278669953346252\n",
            "Epoch [597/1000], Loss: 0.09272246062755585\n",
            "Epoch [598/1000], Loss: 0.09265837073326111\n",
            "Epoch [599/1000], Loss: 0.09259442239999771\n",
            "Epoch [600/1000], Loss: 0.09253060817718506\n",
            "Epoch [601/1000], Loss: 0.09246694296598434\n",
            "Epoch [602/1000], Loss: 0.09240343421697617\n",
            "Epoch [603/1000], Loss: 0.09234004467725754\n",
            "Epoch [604/1000], Loss: 0.09227680414915085\n",
            "Epoch [605/1000], Loss: 0.0922137126326561\n",
            "Epoch [606/1000], Loss: 0.09215076267719269\n",
            "Epoch [607/1000], Loss: 0.09208793938159943\n",
            "Epoch [608/1000], Loss: 0.0920252576470375\n",
            "Epoch [609/1000], Loss: 0.09196271747350693\n",
            "Epoch [610/1000], Loss: 0.09190031886100769\n",
            "Epoch [611/1000], Loss: 0.0918380469083786\n",
            "Epoch [612/1000], Loss: 0.09177591651678085\n",
            "Epoch [613/1000], Loss: 0.09171392768621445\n",
            "Epoch [614/1000], Loss: 0.09165206551551819\n",
            "Epoch [615/1000], Loss: 0.09159033745527267\n",
            "Epoch [616/1000], Loss: 0.0915287509560585\n",
            "Epoch [617/1000], Loss: 0.09146729111671448\n",
            "Epoch [618/1000], Loss: 0.0914059653878212\n",
            "Epoch [619/1000], Loss: 0.09134477376937866\n",
            "Epoch [620/1000], Loss: 0.09128372371196747\n",
            "Epoch [621/1000], Loss: 0.09122280031442642\n",
            "Epoch [622/1000], Loss: 0.09116199612617493\n",
            "Epoch [623/1000], Loss: 0.09110134094953537\n",
            "Epoch [624/1000], Loss: 0.09104080498218536\n",
            "Epoch [625/1000], Loss: 0.0909803956747055\n",
            "Epoch [626/1000], Loss: 0.09092012047767639\n",
            "Epoch [627/1000], Loss: 0.09085996448993683\n",
            "Epoch [628/1000], Loss: 0.09079994261264801\n",
            "Epoch [629/1000], Loss: 0.09074005484580994\n",
            "Epoch [630/1000], Loss: 0.09068028628826141\n",
            "Epoch [631/1000], Loss: 0.09062063694000244\n",
            "Epoch [632/1000], Loss: 0.09056112915277481\n",
            "Epoch [633/1000], Loss: 0.09050174057483673\n",
            "Epoch [634/1000], Loss: 0.0904424786567688\n",
            "Epoch [635/1000], Loss: 0.09038333594799042\n",
            "Epoch [636/1000], Loss: 0.09032431989908218\n",
            "Epoch [637/1000], Loss: 0.0902654230594635\n",
            "Epoch [638/1000], Loss: 0.09020665287971497\n",
            "Epoch [639/1000], Loss: 0.09014801681041718\n",
            "Epoch [640/1000], Loss: 0.09008948504924774\n",
            "Epoch [641/1000], Loss: 0.09003107994794846\n",
            "Epoch [642/1000], Loss: 0.08997280150651932\n",
            "Epoch [643/1000], Loss: 0.08991463482379913\n",
            "Epoch [644/1000], Loss: 0.0898565873503685\n",
            "Epoch [645/1000], Loss: 0.08979866653680801\n",
            "Epoch [646/1000], Loss: 0.08974086493253708\n",
            "Epoch [647/1000], Loss: 0.0896831676363945\n",
            "Epoch [648/1000], Loss: 0.08962561190128326\n",
            "Epoch [649/1000], Loss: 0.08956816047430038\n",
            "Epoch [650/1000], Loss: 0.08951082080602646\n",
            "Epoch [651/1000], Loss: 0.08945361524820328\n",
            "Epoch [652/1000], Loss: 0.08939650654792786\n",
            "Epoch [653/1000], Loss: 0.08933952450752258\n",
            "Epoch [654/1000], Loss: 0.08928266167640686\n",
            "Epoch [655/1000], Loss: 0.0892258957028389\n",
            "Epoch [656/1000], Loss: 0.08916927129030228\n",
            "Epoch [657/1000], Loss: 0.08911273628473282\n",
            "Epoch [658/1000], Loss: 0.08905632793903351\n",
            "Epoch [659/1000], Loss: 0.08900003135204315\n",
            "Epoch [660/1000], Loss: 0.08894385397434235\n",
            "Epoch [661/1000], Loss: 0.0888877734541893\n",
            "Epoch [662/1000], Loss: 0.0888318195939064\n",
            "Epoch [663/1000], Loss: 0.08877597749233246\n",
            "Epoch [664/1000], Loss: 0.08872023224830627\n",
            "Epoch [665/1000], Loss: 0.08866460621356964\n",
            "Epoch [666/1000], Loss: 0.08860909193754196\n",
            "Epoch [667/1000], Loss: 0.08855368196964264\n",
            "Epoch [668/1000], Loss: 0.08849837630987167\n",
            "Epoch [669/1000], Loss: 0.08844319730997086\n",
            "Epoch [670/1000], Loss: 0.0883881226181984\n",
            "Epoch [671/1000], Loss: 0.0883331447839737\n",
            "Epoch [672/1000], Loss: 0.08827827870845795\n",
            "Epoch [673/1000], Loss: 0.08822353184223175\n",
            "Epoch [674/1000], Loss: 0.08816887438297272\n",
            "Epoch [675/1000], Loss: 0.08811432868242264\n",
            "Epoch [676/1000], Loss: 0.08805989474058151\n",
            "Epoch [677/1000], Loss: 0.08800557255744934\n",
            "Epoch [678/1000], Loss: 0.08795134723186493\n",
            "Epoch [679/1000], Loss: 0.08789721876382828\n",
            "Epoch [680/1000], Loss: 0.08784320205450058\n",
            "Epoch [681/1000], Loss: 0.08778929710388184\n",
            "Epoch [682/1000], Loss: 0.08773549646139145\n",
            "Epoch [683/1000], Loss: 0.08768179267644882\n",
            "Epoch [684/1000], Loss: 0.08762819319963455\n",
            "Epoch [685/1000], Loss: 0.08757469803094864\n",
            "Epoch [686/1000], Loss: 0.08752129971981049\n",
            "Epoch [687/1000], Loss: 0.08746800571680069\n",
            "Epoch [688/1000], Loss: 0.08741481602191925\n",
            "Epoch [689/1000], Loss: 0.08736173808574677\n",
            "Epoch [690/1000], Loss: 0.08730874210596085\n",
            "Epoch [691/1000], Loss: 0.08725585788488388\n",
            "Epoch [692/1000], Loss: 0.08720307052135468\n",
            "Epoch [693/1000], Loss: 0.08715038001537323\n",
            "Epoch [694/1000], Loss: 0.08709778636693954\n",
            "Epoch [695/1000], Loss: 0.08704530447721481\n",
            "Epoch [696/1000], Loss: 0.08699291199445724\n",
            "Epoch [697/1000], Loss: 0.08694062381982803\n",
            "Epoch [698/1000], Loss: 0.08688843250274658\n",
            "Epoch [699/1000], Loss: 0.08683633804321289\n",
            "Epoch [700/1000], Loss: 0.08678433299064636\n",
            "Epoch [701/1000], Loss: 0.08673243969678879\n",
            "Epoch [702/1000], Loss: 0.08668062090873718\n",
            "Epoch [703/1000], Loss: 0.08662892132997513\n",
            "Epoch [704/1000], Loss: 0.08657730370759964\n",
            "Epoch [705/1000], Loss: 0.0865257978439331\n",
            "Epoch [706/1000], Loss: 0.08647436648607254\n",
            "Epoch [707/1000], Loss: 0.08642304688692093\n",
            "Epoch [708/1000], Loss: 0.08637181669473648\n",
            "Epoch [709/1000], Loss: 0.08632068336009979\n",
            "Epoch [710/1000], Loss: 0.08626963943243027\n",
            "Epoch [711/1000], Loss: 0.0862186923623085\n",
            "Epoch [712/1000], Loss: 0.0861678346991539\n",
            "Epoch [713/1000], Loss: 0.08611706644296646\n",
            "Epoch [714/1000], Loss: 0.08606640994548798\n",
            "Epoch [715/1000], Loss: 0.08601582795381546\n",
            "Epoch [716/1000], Loss: 0.0859653428196907\n",
            "Epoch [717/1000], Loss: 0.08591495454311371\n",
            "Epoch [718/1000], Loss: 0.08586464822292328\n",
            "Epoch [719/1000], Loss: 0.08581443876028061\n",
            "Epoch [720/1000], Loss: 0.0857643187046051\n",
            "Epoch [721/1000], Loss: 0.08571428805589676\n",
            "Epoch [722/1000], Loss: 0.08566434681415558\n",
            "Epoch [723/1000], Loss: 0.08561450242996216\n",
            "Epoch [724/1000], Loss: 0.0855647474527359\n",
            "Epoch [725/1000], Loss: 0.0855150818824768\n",
            "Epoch [726/1000], Loss: 0.08546549826860428\n",
            "Epoch [727/1000], Loss: 0.08541601151227951\n",
            "Epoch [728/1000], Loss: 0.08536659926176071\n",
            "Epoch [729/1000], Loss: 0.08531729131937027\n",
            "Epoch [730/1000], Loss: 0.0852680653333664\n",
            "Epoch [731/1000], Loss: 0.08521892130374908\n",
            "Epoch [732/1000], Loss: 0.08516986668109894\n",
            "Epoch [733/1000], Loss: 0.08512090146541595\n",
            "Epoch [734/1000], Loss: 0.08507202565670013\n",
            "Epoch [735/1000], Loss: 0.08502322435379028\n",
            "Epoch [736/1000], Loss: 0.08497451990842819\n",
            "Epoch [737/1000], Loss: 0.08492589741945267\n",
            "Epoch [738/1000], Loss: 0.0848773643374443\n",
            "Epoch [739/1000], Loss: 0.08482891321182251\n",
            "Epoch [740/1000], Loss: 0.08478055149316788\n",
            "Epoch [741/1000], Loss: 0.08473227173089981\n",
            "Epoch [742/1000], Loss: 0.08468407392501831\n",
            "Epoch [743/1000], Loss: 0.08463595807552338\n",
            "Epoch [744/1000], Loss: 0.0845879316329956\n",
            "Epoch [745/1000], Loss: 0.084539994597435\n",
            "Epoch [746/1000], Loss: 0.08449213206768036\n",
            "Epoch [747/1000], Loss: 0.08444434404373169\n",
            "Epoch [748/1000], Loss: 0.08439665287733078\n",
            "Epoch [749/1000], Loss: 0.08434903621673584\n",
            "Epoch [750/1000], Loss: 0.08430151641368866\n",
            "Epoch [751/1000], Loss: 0.08425406366586685\n",
            "Epoch [752/1000], Loss: 0.08420668542385101\n",
            "Epoch [753/1000], Loss: 0.08415940403938293\n",
            "Epoch [754/1000], Loss: 0.08411220461130142\n",
            "Epoch [755/1000], Loss: 0.08406507968902588\n",
            "Epoch [756/1000], Loss: 0.0840180292725563\n",
            "Epoch [757/1000], Loss: 0.08397107571363449\n",
            "Epoch [758/1000], Loss: 0.08392418175935745\n",
            "Epoch [759/1000], Loss: 0.08387738466262817\n",
            "Epoch [760/1000], Loss: 0.08383066207170486\n",
            "Epoch [761/1000], Loss: 0.08378402143716812\n",
            "Epoch [762/1000], Loss: 0.08373744785785675\n",
            "Epoch [763/1000], Loss: 0.08369096368551254\n",
            "Epoch [764/1000], Loss: 0.0836445614695549\n",
            "Epoch [765/1000], Loss: 0.08359823375940323\n",
            "Epoch [766/1000], Loss: 0.08355197310447693\n",
            "Epoch [767/1000], Loss: 0.08350580185651779\n",
            "Epoch [768/1000], Loss: 0.08345970511436462\n",
            "Epoch [769/1000], Loss: 0.08341368287801743\n",
            "Epoch [770/1000], Loss: 0.0833677425980568\n",
            "Epoch [771/1000], Loss: 0.08332186937332153\n",
            "Epoch [772/1000], Loss: 0.08327608555555344\n",
            "Epoch [773/1000], Loss: 0.08323037624359131\n",
            "Epoch [774/1000], Loss: 0.08318474143743515\n",
            "Epoch [775/1000], Loss: 0.08313918113708496\n",
            "Epoch [776/1000], Loss: 0.08309369534254074\n",
            "Epoch [777/1000], Loss: 0.08304828405380249\n",
            "Epoch [778/1000], Loss: 0.08300294727087021\n",
            "Epoch [779/1000], Loss: 0.0829576924443245\n",
            "Epoch [780/1000], Loss: 0.08291251212358475\n",
            "Epoch [781/1000], Loss: 0.08286739885807037\n",
            "Epoch [782/1000], Loss: 0.08282235264778137\n",
            "Epoch [783/1000], Loss: 0.08277739584445953\n",
            "Epoch [784/1000], Loss: 0.08273249864578247\n",
            "Epoch [785/1000], Loss: 0.08268768340349197\n",
            "Epoch [786/1000], Loss: 0.08264295011758804\n",
            "Epoch [787/1000], Loss: 0.08259828388690948\n",
            "Epoch [788/1000], Loss: 0.0825536847114563\n",
            "Epoch [789/1000], Loss: 0.08250916749238968\n",
            "Epoch [790/1000], Loss: 0.08246470987796783\n",
            "Epoch [791/1000], Loss: 0.08242033421993256\n",
            "Epoch [792/1000], Loss: 0.08237602561712265\n",
            "Epoch [793/1000], Loss: 0.08233179152011871\n",
            "Epoch [794/1000], Loss: 0.08228762447834015\n",
            "Epoch [795/1000], Loss: 0.08224353939294815\n",
            "Epoch [796/1000], Loss: 0.08219951391220093\n",
            "Epoch [797/1000], Loss: 0.08215556293725967\n",
            "Epoch [798/1000], Loss: 0.08211169391870499\n",
            "Epoch [799/1000], Loss: 0.08206787705421448\n",
            "Epoch [800/1000], Loss: 0.08202413469552994\n",
            "Epoch [801/1000], Loss: 0.08198046684265137\n",
            "Epoch [802/1000], Loss: 0.08193686604499817\n",
            "Epoch [803/1000], Loss: 0.08189333975315094\n",
            "Epoch [804/1000], Loss: 0.08184988051652908\n",
            "Epoch [805/1000], Loss: 0.0818064883351326\n",
            "Epoch [806/1000], Loss: 0.08176316320896149\n",
            "Epoch [807/1000], Loss: 0.08171991258859634\n",
            "Epoch [808/1000], Loss: 0.08167672902345657\n",
            "Epoch [809/1000], Loss: 0.08163361251354218\n",
            "Epoch [810/1000], Loss: 0.08159056305885315\n",
            "Epoch [811/1000], Loss: 0.0815475806593895\n",
            "Epoch [812/1000], Loss: 0.08150467276573181\n",
            "Epoch [813/1000], Loss: 0.0814618244767189\n",
            "Epoch [814/1000], Loss: 0.08141905069351196\n",
            "Epoch [815/1000], Loss: 0.0813763290643692\n",
            "Epoch [816/1000], Loss: 0.081333689391613\n",
            "Epoch [817/1000], Loss: 0.08129110932350159\n",
            "Epoch [818/1000], Loss: 0.08124860376119614\n",
            "Epoch [819/1000], Loss: 0.08120615780353546\n",
            "Epoch [820/1000], Loss: 0.08116377890110016\n",
            "Epoch [821/1000], Loss: 0.08112147450447083\n",
            "Epoch [822/1000], Loss: 0.08107922971248627\n",
            "Epoch [823/1000], Loss: 0.08103703707456589\n",
            "Epoch [824/1000], Loss: 0.08099492639303207\n",
            "Epoch [825/1000], Loss: 0.08095286786556244\n",
            "Epoch [826/1000], Loss: 0.08091089129447937\n",
            "Epoch [827/1000], Loss: 0.08086896687746048\n",
            "Epoch [828/1000], Loss: 0.08082710951566696\n",
            "Epoch [829/1000], Loss: 0.08078532665967941\n",
            "Epoch [830/1000], Loss: 0.08074359595775604\n",
            "Epoch [831/1000], Loss: 0.08070193976163864\n",
            "Epoch [832/1000], Loss: 0.08066032826900482\n",
            "Epoch [833/1000], Loss: 0.08061879128217697\n",
            "Epoch [834/1000], Loss: 0.0805773213505745\n",
            "Epoch [835/1000], Loss: 0.08053591847419739\n",
            "Epoch [836/1000], Loss: 0.08049456775188446\n",
            "Epoch [837/1000], Loss: 0.0804532840847969\n",
            "Epoch [838/1000], Loss: 0.08041206747293472\n",
            "Epoch [839/1000], Loss: 0.08037091046571732\n",
            "Epoch [840/1000], Loss: 0.08032980561256409\n",
            "Epoch [841/1000], Loss: 0.08028878271579742\n",
            "Epoch [842/1000], Loss: 0.08024780452251434\n",
            "Epoch [843/1000], Loss: 0.08020690083503723\n",
            "Epoch [844/1000], Loss: 0.0801660493016243\n",
            "Epoch [845/1000], Loss: 0.08012526482343674\n",
            "Epoch [846/1000], Loss: 0.08008453994989395\n",
            "Epoch [847/1000], Loss: 0.08004387468099594\n",
            "Epoch [848/1000], Loss: 0.0800032764673233\n",
            "Epoch [849/1000], Loss: 0.07996273785829544\n",
            "Epoch [850/1000], Loss: 0.07992225140333176\n",
            "Epoch [851/1000], Loss: 0.07988183945417404\n",
            "Epoch [852/1000], Loss: 0.07984147220849991\n",
            "Epoch [853/1000], Loss: 0.07980117946863174\n",
            "Epoch [854/1000], Loss: 0.07976093888282776\n",
            "Epoch [855/1000], Loss: 0.07972075045108795\n",
            "Epoch [856/1000], Loss: 0.07968063652515411\n",
            "Epoch [857/1000], Loss: 0.07964057475328445\n",
            "Epoch [858/1000], Loss: 0.07960057258605957\n",
            "Epoch [859/1000], Loss: 0.07956063002347946\n",
            "Epoch [860/1000], Loss: 0.07952073961496353\n",
            "Epoch [861/1000], Loss: 0.07948092371225357\n",
            "Epoch [862/1000], Loss: 0.07944115251302719\n",
            "Epoch [863/1000], Loss: 0.07940144836902618\n",
            "Epoch [864/1000], Loss: 0.07936178892850876\n",
            "Epoch [865/1000], Loss: 0.0793222114443779\n",
            "Epoch [866/1000], Loss: 0.07928267866373062\n",
            "Epoch [867/1000], Loss: 0.07924319803714752\n",
            "Epoch [868/1000], Loss: 0.0792037844657898\n",
            "Epoch [869/1000], Loss: 0.07916442304849625\n",
            "Epoch [870/1000], Loss: 0.07912512123584747\n",
            "Epoch [871/1000], Loss: 0.07908587157726288\n",
            "Epoch [872/1000], Loss: 0.07904668897390366\n",
            "Epoch [873/1000], Loss: 0.07900755852460861\n",
            "Epoch [874/1000], Loss: 0.07896848022937775\n",
            "Epoch [875/1000], Loss: 0.07892946153879166\n",
            "Epoch [876/1000], Loss: 0.07889050990343094\n",
            "Epoch [877/1000], Loss: 0.0788515955209732\n",
            "Epoch [878/1000], Loss: 0.07881274819374084\n",
            "Epoch [879/1000], Loss: 0.07877396047115326\n",
            "Epoch [880/1000], Loss: 0.07873521745204926\n",
            "Epoch [881/1000], Loss: 0.07869653403759003\n",
            "Epoch [882/1000], Loss: 0.07865791767835617\n",
            "Epoch [883/1000], Loss: 0.0786193460226059\n",
            "Epoch [884/1000], Loss: 0.0785808265209198\n",
            "Epoch [885/1000], Loss: 0.07854235917329788\n",
            "Epoch [886/1000], Loss: 0.07850395888090134\n",
            "Epoch [887/1000], Loss: 0.07846561074256897\n",
            "Epoch [888/1000], Loss: 0.07842730730772018\n",
            "Epoch [889/1000], Loss: 0.07838907092809677\n",
            "Epoch [890/1000], Loss: 0.07835087925195694\n",
            "Epoch [891/1000], Loss: 0.07831274718046188\n",
            "Epoch [892/1000], Loss: 0.078274667263031\n",
            "Epoch [893/1000], Loss: 0.0782366544008255\n",
            "Epoch [894/1000], Loss: 0.07819867134094238\n",
            "Epoch [895/1000], Loss: 0.07816075533628464\n",
            "Epoch [896/1000], Loss: 0.07812289148569107\n",
            "Epoch [897/1000], Loss: 0.07808508723974228\n",
            "Epoch [898/1000], Loss: 0.07804732024669647\n",
            "Epoch [899/1000], Loss: 0.07800962030887604\n",
            "Epoch [900/1000], Loss: 0.07797196507453918\n",
            "Epoch [901/1000], Loss: 0.0779343768954277\n",
            "Epoch [902/1000], Loss: 0.07789682596921921\n",
            "Epoch [903/1000], Loss: 0.07785933464765549\n",
            "Epoch [904/1000], Loss: 0.07782189548015594\n",
            "Epoch [905/1000], Loss: 0.07778450101613998\n",
            "Epoch [906/1000], Loss: 0.0777471661567688\n",
            "Epoch [907/1000], Loss: 0.0777098760008812\n",
            "Epoch [908/1000], Loss: 0.07767263799905777\n",
            "Epoch [909/1000], Loss: 0.07763546705245972\n",
            "Epoch [910/1000], Loss: 0.07759833335876465\n",
            "Epoch [911/1000], Loss: 0.07756125926971436\n",
            "Epoch [912/1000], Loss: 0.07752423733472824\n",
            "Epoch [913/1000], Loss: 0.07748725265264511\n",
            "Epoch [914/1000], Loss: 0.07745033502578735\n",
            "Epoch [915/1000], Loss: 0.07741344720125198\n",
            "Epoch [916/1000], Loss: 0.07737663388252258\n",
            "Epoch [917/1000], Loss: 0.07733986526727676\n",
            "Epoch [918/1000], Loss: 0.07730314135551453\n",
            "Epoch [919/1000], Loss: 0.07726646959781647\n",
            "Epoch [920/1000], Loss: 0.07722984999418259\n",
            "Epoch [921/1000], Loss: 0.07719327509403229\n",
            "Epoch [922/1000], Loss: 0.07715675979852676\n",
            "Epoch [923/1000], Loss: 0.07712028175592422\n",
            "Epoch [924/1000], Loss: 0.07708386331796646\n",
            "Epoch [925/1000], Loss: 0.07704748213291168\n",
            "Epoch [926/1000], Loss: 0.07701116800308228\n",
            "Epoch [927/1000], Loss: 0.07697489857673645\n",
            "Epoch [928/1000], Loss: 0.07693866640329361\n",
            "Epoch [929/1000], Loss: 0.07690249383449554\n",
            "Epoch [930/1000], Loss: 0.07686636596918106\n",
            "Epoch [931/1000], Loss: 0.07683029770851135\n",
            "Epoch [932/1000], Loss: 0.07679425179958344\n",
            "Epoch [933/1000], Loss: 0.07675828039646149\n",
            "Epoch [934/1000], Loss: 0.07672235369682312\n",
            "Epoch [935/1000], Loss: 0.07668646425008774\n",
            "Epoch [936/1000], Loss: 0.07665064185857773\n",
            "Epoch [937/1000], Loss: 0.07661484181880951\n",
            "Epoch [938/1000], Loss: 0.07657911628484726\n",
            "Epoch [939/1000], Loss: 0.0765434205532074\n",
            "Epoch [940/1000], Loss: 0.07650776952505112\n",
            "Epoch [941/1000], Loss: 0.07647218555212021\n",
            "Epoch [942/1000], Loss: 0.07643663138151169\n",
            "Epoch [943/1000], Loss: 0.07640114426612854\n",
            "Epoch [944/1000], Loss: 0.07636567950248718\n",
            "Epoch [945/1000], Loss: 0.0763302817940712\n",
            "Epoch [946/1000], Loss: 0.0762949213385582\n",
            "Epoch [947/1000], Loss: 0.07625959813594818\n",
            "Epoch [948/1000], Loss: 0.07622434198856354\n",
            "Epoch [949/1000], Loss: 0.07618912309408188\n",
            "Epoch [950/1000], Loss: 0.0761539489030838\n",
            "Epoch [951/1000], Loss: 0.0761188268661499\n",
            "Epoch [952/1000], Loss: 0.07608374953269958\n",
            "Epoch [953/1000], Loss: 0.07604870945215225\n",
            "Epoch [954/1000], Loss: 0.0760137215256691\n",
            "Epoch [955/1000], Loss: 0.07597878575325012\n",
            "Epoch [956/1000], Loss: 0.07594388723373413\n",
            "Epoch [957/1000], Loss: 0.07590904831886292\n",
            "Epoch [958/1000], Loss: 0.07587423920631409\n",
            "Epoch [959/1000], Loss: 0.07583948224782944\n",
            "Epoch [960/1000], Loss: 0.07580477744340897\n",
            "Epoch [961/1000], Loss: 0.07577010989189148\n",
            "Epoch [962/1000], Loss: 0.07573548704385757\n",
            "Epoch [963/1000], Loss: 0.07570090144872665\n",
            "Epoch [964/1000], Loss: 0.07566636800765991\n",
            "Epoch [965/1000], Loss: 0.07563188672065735\n",
            "Epoch [966/1000], Loss: 0.07559744268655777\n",
            "Epoch [967/1000], Loss: 0.07556304335594177\n",
            "Epoch [968/1000], Loss: 0.07552869617938995\n",
            "Epoch [969/1000], Loss: 0.07549438625574112\n",
            "Epoch [970/1000], Loss: 0.07546012103557587\n",
            "Epoch [971/1000], Loss: 0.07542590796947479\n",
            "Epoch [972/1000], Loss: 0.0753917321562767\n",
            "Epoch [973/1000], Loss: 0.0753575935959816\n",
            "Epoch [974/1000], Loss: 0.07532352209091187\n",
            "Epoch [975/1000], Loss: 0.07528947293758392\n",
            "Epoch [976/1000], Loss: 0.07525546848773956\n",
            "Epoch [977/1000], Loss: 0.07522151619195938\n",
            "Epoch [978/1000], Loss: 0.07518759369850159\n",
            "Epoch [979/1000], Loss: 0.07515373826026917\n",
            "Epoch [980/1000], Loss: 0.07511990517377853\n",
            "Epoch [981/1000], Loss: 0.07508613169193268\n",
            "Epoch [982/1000], Loss: 0.07505238801240921\n",
            "Epoch [983/1000], Loss: 0.07501869648694992\n",
            "Epoch [984/1000], Loss: 0.07498503476381302\n",
            "Epoch [985/1000], Loss: 0.0749514251947403\n",
            "Epoch [986/1000], Loss: 0.07491786032915115\n",
            "Epoch [987/1000], Loss: 0.074884332716465\n",
            "Epoch [988/1000], Loss: 0.07485085725784302\n",
            "Epoch [989/1000], Loss: 0.07481741160154343\n",
            "Epoch [990/1000], Loss: 0.07478401064872742\n",
            "Epoch [991/1000], Loss: 0.07475066184997559\n",
            "Epoch [992/1000], Loss: 0.07471734285354614\n",
            "Epoch [993/1000], Loss: 0.07468406856060028\n",
            "Epoch [994/1000], Loss: 0.0746508464217186\n",
            "Epoch [995/1000], Loss: 0.0746176540851593\n",
            "Epoch [996/1000], Loss: 0.07458450645208359\n",
            "Epoch [997/1000], Loss: 0.07455140352249146\n",
            "Epoch [998/1000], Loss: 0.07451833784580231\n",
            "Epoch [999/1000], Loss: 0.07448531687259674\n",
            "Epoch [1000/1000], Loss: 0.07445233315229416\n",
            "Test Loss: 0.09691587835550308\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the linear classifier model\n",
        "class LinearClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the linear classifier\n",
        "input_size = all_features.shape[1]\n",
        "classifier = LinearClassifier(input_size).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
        "\n",
        "# Training loop\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = classifier(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_outputs = classifier(X_test_tensor)\n",
        "    test_loss = criterion(test_outputs, y_test_tensor)\n",
        "    print(f'Test Loss: {test_loss.item()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcik-P4U91Wr",
        "outputId": "ec926571-0520-4fca-bb73-81104e1e5c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 768)\n"
          ]
        }
      ],
      "source": [
        "fake_features = feat_dict.values()\n",
        "fake_features = list(fake_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4H123oJ9W2v",
        "outputId": "b3b0068e-da9d-4fb3-f84a-626ec71ba7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 1700)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "fake_data = []\n",
        "y_data = []\n",
        "\n",
        "# Storing images' encoding in a matrix\n",
        "for class_data in enumerate(fake_features):\n",
        "  temp_feat = np.array(class_data[1])\n",
        "  temp_label = np.array([1 for i in range(len(class_data[1]))])\n",
        "  for j in range(len(fake_features)):\n",
        "    if j != class_data[0]:\n",
        "      random_ele = np.array(random.sample(fake_features[j], len(class_data[1])//len(uniq_class)))\n",
        "      random_label = np.array([0 for i in range(len(class_data[1])//len(uniq_class))])\n",
        "      temp_feat = np.concatenate((temp_feat, random_ele))\n",
        "      temp_label = np.concatenate((temp_label, random_label))\n",
        "  fake_data.append(np.matrix(temp_feat))\n",
        "  y_data.append(temp_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMQdBbC6ULwp"
      },
      "source": [
        "Using encoded images to create test, train and validate dataset from 4 generative models to be used for chain classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZihYLreIuAm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "fake_datasets = fake_data\n",
        "y_datasets = y_data\n",
        "num_datasets = len(fake_datasets)\n",
        "X_train_datasets = []\n",
        "X_test_datasets = []\n",
        "y_train_datasets = []\n",
        "y_test_datasets = []\n",
        "\n",
        "# Diving into test, train and validate sets\n",
        "seed_value = 42\n",
        "for i in range(num_datasets):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        fake_datasets[i], y_datasets[i], test_size=0.2, random_state=seed_value\n",
        "    )\n",
        "    X_train_datasets.append(np.array(X_train))\n",
        "    X_test_datasets.append(np.array(X_test))\n",
        "    y_train_datasets.append(np.array(y_train))\n",
        "    y_test_datasets.append(np.array(y_test))\n",
        "    seed_value += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixLfIaJFVCWu"
      },
      "source": [
        "In the following cells SVMs, NN, Random forests were trained on these datasets from different models for chain classification, each model is trained only on datasets from 1 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1rPVTQEOokw",
        "outputId": "133b8c37-cf49-4a67-ce9a-448b190759bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Dataset 1: 0.7588235294117647\n",
            "Accuracy for Dataset 2: 0.7970588235294118\n",
            "Accuracy for Dataset 3: 0.7147058823529412\n",
            "Accuracy for Dataset 4: 0.8235294117647058\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X1_scaled = scaler.fit_transform(X_train_datasets[4])\n",
        "X2_scaled = scaler.fit_transform(X_train_datasets[5])\n",
        "X3_scaled = scaler.fit_transform(X_train_datasets[6])\n",
        "X4_scaled = scaler.fit_transform(X_train_datasets[7])\n",
        "\n",
        "# Create SVM instances for each dataset\n",
        "svm1 = svm.SVC(kernel='sigmoid')\n",
        "svm2 = svm.SVC(kernel='sigmoid')\n",
        "svm3 = svm.SVC(kernel='sigmoid')\n",
        "svm4 = svm.SVC(kernel='sigmoid')\n",
        "\n",
        "# Train SVM models on each dataset\n",
        "svm1.fit(X1_scaled, y_train_datasets[4])\n",
        "svm2.fit(X2_scaled, y_train_datasets[5])\n",
        "svm3.fit(X3_scaled, y_train_datasets[6])\n",
        "svm4.fit(X4_scaled, y_train_datasets[7])\n",
        "\n",
        "y1_pred = svm1.predict(scaler.transform(X_test_datasets[4]))\n",
        "y2_pred = svm2.predict(scaler.transform(X_test_datasets[5]))\n",
        "y3_pred = svm3.predict(scaler.transform(X_test_datasets[6]))\n",
        "y4_pred = svm4.predict(scaler.transform(X_test_datasets[7]))\n",
        "\n",
        "# Evaluate accuracy\n",
        "acc1 = accuracy_score(y_test_datasets[4], y1_pred)\n",
        "acc2 = accuracy_score(y_test_datasets[5], y2_pred)\n",
        "acc3 = accuracy_score(y_test_datasets[6], y3_pred)\n",
        "acc4 = accuracy_score(y_test_datasets[7], y4_pred)\n",
        "\n",
        "# Display results\n",
        "print(f\"Accuracy for Dataset 1: {acc1}\")\n",
        "print(f\"Accuracy for Dataset 2: {acc2}\")\n",
        "print(f\"Accuracy for Dataset 3: {acc3}\")\n",
        "print(f\"Accuracy for Dataset 4: {acc4}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC3CDuy7Ldgj",
        "outputId": "336499fd-7aa9-4187-edc3-3d0cb7dfd4b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Random Forest 1: 0.7235294117647059\n",
            "Accuracy for Random Forest 2: 0.55\n",
            "Accuracy for Random Forest 3: 0.55\n",
            "Accuracy for Random Forest 4: 0.5882352941176471\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train four random forests with different settings\n",
        "rf1 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "rf2 = RandomForestClassifier(n_estimators=100, random_state=2)\n",
        "rf3 = RandomForestClassifier(n_estimators=150, random_state=3)\n",
        "rf4 = RandomForestClassifier(n_estimators=200, random_state=4)\n",
        "\n",
        "# Train the models\n",
        "rf1.fit(X_train_datasets[4], y_train_datasets[4])\n",
        "rf2.fit(X_train_datasets[5], y_train_datasets[5])\n",
        "rf3.fit(X_train_datasets[6], y_train_datasets[6])\n",
        "rf4.fit(X_train_datasets[7], y_train_datasets[7])\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred1 = rf1.predict(X_test_datasets[4])\n",
        "y_pred2 = rf2.predict(X_test_datasets[5])\n",
        "y_pred3 = rf3.predict(X_test_datasets[6])\n",
        "y_pred4 = rf4.predict(X_test_datasets[7])\n",
        "\n",
        "# Evaluate the models\n",
        "acc1 = accuracy_score(y_test_datasets[4], y_pred1)\n",
        "acc2 = accuracy_score(y_test_datasets[4], y_pred2)\n",
        "acc3 = accuracy_score(y_test_datasets[4], y_pred3)\n",
        "acc4 = accuracy_score(y_test_datasets[4], y_pred4)\n",
        "\n",
        "print(\"Accuracy for Random Forest 1:\", acc1)\n",
        "print(\"Accuracy for Random Forest 2:\", acc2)\n",
        "print(\"Accuracy for Random Forest 3:\", acc3)\n",
        "print(\"Accuracy for Random Forest 4:\", acc4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axzMLcb0NuIv",
        "outputId": "62c054c7-beb7-40e8-aad9-5616558cb5ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "34/34 [==============================] - 2s 16ms/step - loss: 0.6584 - accuracy: 0.6608 - val_loss: 0.5496 - val_accuracy: 0.7390\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 0s 9ms/step - loss: 0.4666 - accuracy: 0.8006 - val_loss: 0.5304 - val_accuracy: 0.7537\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 0s 11ms/step - loss: 0.3891 - accuracy: 0.8346 - val_loss: 0.5189 - val_accuracy: 0.7500\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 0.3195 - accuracy: 0.8778 - val_loss: 0.5369 - val_accuracy: 0.7684\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 8ms/step - loss: 0.2570 - accuracy: 0.9062 - val_loss: 0.5524 - val_accuracy: 0.7647\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 0s 14ms/step - loss: 0.1857 - accuracy: 0.9357 - val_loss: 0.6775 - val_accuracy: 0.7132\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 0.1412 - accuracy: 0.9577 - val_loss: 0.6604 - val_accuracy: 0.7426\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 0.0788 - accuracy: 0.9881 - val_loss: 0.7020 - val_accuracy: 0.7426\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9917 - val_loss: 0.8779 - val_accuracy: 0.6912\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 0.0297 - accuracy: 0.9991 - val_loss: 0.8645 - val_accuracy: 0.7279\n",
            "Epoch 1/10\n",
            "34/34 [==============================] - 1s 9ms/step - loss: 0.6486 - accuracy: 0.6774 - val_loss: 0.5555 - val_accuracy: 0.7169\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.8180 - val_loss: 0.5288 - val_accuracy: 0.7316\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.3395 - accuracy: 0.8686 - val_loss: 0.5294 - val_accuracy: 0.7426\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.2657 - accuracy: 0.9053 - val_loss: 0.5684 - val_accuracy: 0.7390\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 0.2303 - accuracy: 0.9173 - val_loss: 0.6125 - val_accuracy: 0.6801\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.9577 - val_loss: 0.6121 - val_accuracy: 0.7537\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9798 - val_loss: 0.7181 - val_accuracy: 0.7059\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 0.0707 - accuracy: 0.9908 - val_loss: 0.7607 - val_accuracy: 0.7353\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 0.0493 - accuracy: 0.9963 - val_loss: 0.7731 - val_accuracy: 0.7610\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 0s 6ms/step - loss: 0.0304 - accuracy: 0.9982 - val_loss: 0.8208 - val_accuracy: 0.7206\n",
            "Epoch 1/10\n",
            "34/34 [==============================] - 2s 12ms/step - loss: 0.5836 - accuracy: 0.6875 - val_loss: 0.4891 - val_accuracy: 0.7684\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 0.3314 - accuracy: 0.8640 - val_loss: 0.3918 - val_accuracy: 0.8309\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 0s 7ms/step - loss: 0.2304 - accuracy: 0.9127 - val_loss: 0.3490 - val_accuracy: 0.8640\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 0s 5ms/step - loss: 0.1534 - accuracy: 0.9476 - val_loss: 0.3557 - val_accuracy: 0.8824\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9733 - val_loss: 0.3629 - val_accuracy: 0.8676\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0568 - accuracy: 0.9926 - val_loss: 0.3916 - val_accuracy: 0.8713\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9926 - val_loss: 0.3848 - val_accuracy: 0.8787\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.4165 - val_accuracy: 0.8787\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.4441 - val_accuracy: 0.8860\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.4629 - val_accuracy: 0.8860\n",
            "Epoch 1/10\n",
            "34/34 [==============================] - 1s 8ms/step - loss: 0.5954 - accuracy: 0.6967 - val_loss: 0.4716 - val_accuracy: 0.8015\n",
            "Epoch 2/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.8024 - val_loss: 0.4443 - val_accuracy: 0.8015\n",
            "Epoch 3/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.3745 - accuracy: 0.8401 - val_loss: 0.4389 - val_accuracy: 0.8051\n",
            "Epoch 4/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.2929 - accuracy: 0.8814 - val_loss: 0.4323 - val_accuracy: 0.8419\n",
            "Epoch 5/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.2280 - accuracy: 0.9145 - val_loss: 0.5349 - val_accuracy: 0.7132\n",
            "Epoch 6/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.1800 - accuracy: 0.9449 - val_loss: 0.4745 - val_accuracy: 0.8456\n",
            "Epoch 7/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.1350 - accuracy: 0.9614 - val_loss: 0.5268 - val_accuracy: 0.7610\n",
            "Epoch 8/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0871 - accuracy: 0.9881 - val_loss: 0.5365 - val_accuracy: 0.7868\n",
            "Epoch 9/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0522 - accuracy: 0.9936 - val_loss: 0.5472 - val_accuracy: 0.7941\n",
            "Epoch 10/10\n",
            "34/34 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9982 - val_loss: 0.5918 - val_accuracy: 0.8088\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "Accuracy for Neural Network 1: 0.7058823529411765\n",
            "Accuracy for Neural Network 2: 0.5088235294117647\n",
            "Accuracy for Neural Network 3: 0.5235294117647059\n",
            "Accuracy for Neural Network 4: 0.5323529411764706\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def create_neural_network():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_datasets[4].shape[1],)),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')  # Assuming you have 3 classes for classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn1 = create_neural_network()\n",
        "nn2 = create_neural_network()\n",
        "nn3 = create_neural_network()\n",
        "nn4 = create_neural_network()\n",
        "\n",
        "# Training\n",
        "nn1.fit(X_train_datasets[4], y_train_datasets[4], epochs=10, batch_size=32, validation_split=0.2)\n",
        "nn2.fit(X_train_datasets[5], y_train_datasets[5], epochs=10, batch_size=32, validation_split=0.2)\n",
        "nn3.fit(X_train_datasets[6], y_train_datasets[6], epochs=10, batch_size=32, validation_split=0.2)\n",
        "nn4.fit(X_train_datasets[7], y_train_datasets[7], epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_prob_nn1 = nn1.predict(X_test_datasets[4])\n",
        "y_pred_prob_nn2 = nn2.predict(X_test_datasets[5])\n",
        "y_pred_prob_nn3 = nn3.predict(X_test_datasets[6])\n",
        "y_pred_prob_nn4 = nn4.predict(X_test_datasets[7])\n",
        "\n",
        "# Get the predicted classes\n",
        "y_pred_nn1 = tf.argmax(y_pred_prob_nn1, axis=1)\n",
        "y_pred_nn2 = tf.argmax(y_pred_prob_nn2, axis=1)\n",
        "y_pred_nn3 = tf.argmax(y_pred_prob_nn3, axis=1)\n",
        "y_pred_nn4 = tf.argmax(y_pred_prob_nn4, axis=1)\n",
        "\n",
        "# Evaluate the models\n",
        "acc_nn1 = accuracy_score(y_test_datasets[4], y_pred_nn1)\n",
        "acc_nn2 = accuracy_score(y_test_datasets[4], y_pred_nn2)\n",
        "acc_nn3 = accuracy_score(y_test_datasets[4], y_pred_nn3)\n",
        "acc_nn4 = accuracy_score(y_test_datasets[4], y_pred_nn4)\n",
        "\n",
        "print(\"Accuracy for Neural Network 1:\", acc_nn1)\n",
        "print(\"Accuracy for Neural Network 2:\", acc_nn2)\n",
        "print(\"Accuracy for Neural Network 3:\", acc_nn3)\n",
        "print(\"Accuracy for Neural Network 4:\", acc_nn4)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
